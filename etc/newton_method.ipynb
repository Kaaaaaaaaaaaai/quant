{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a86b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "    \n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfe319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.hello-statisticians.com/optimization/optimization5.html\n",
    "# https://www.hello-statisticians.com/optimization/optimization4.html\n",
    "\n",
    "@jax.jit\n",
    "def objective(x:jnp.ndarray) -> float:\n",
    "    \n",
    "    return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "@jax.jit\n",
    "def constraints(x:jnp.ndarray) -> jnp.ndarray:\n",
    "\n",
    "    return jnp.asarray([\n",
    "        (x[0]**2 + x[1]**2 - 2),\n",
    "        (-x[0] + x[1]),\n",
    "        (-x[1])\n",
    "        ])\n",
    "\n",
    "@jax.jit\n",
    "def lagrange(x:jnp.ndarray) -> float:\n",
    "\n",
    "    \n",
    "    return objective(x[:-3]) + x[-3:] @ constraints(x[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b5bb7aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 59 iterations.: [0.99999847 1.99999694]\n",
      "\n",
      "Iteration 0: x = [1.2001163  1.12023456], constraints = [ 0.69520459 -0.07988174 -1.12023456], objective = 0.8140337686998983\n",
      "Iteration 1: x = [1.18618927 1.10897396], constraints = [ 0.63686823 -0.07721531 -1.10897396], objective = 0.8285938505142163\n",
      "Iteration 2: x = [1.16997149 1.09755232], constraints = [ 0.5734544  -0.07241917 -1.09755232], objective = 0.8433021145854976\n",
      "Iteration 3: x = [1.1625937  1.09107278], constraints = [ 0.54206394 -0.07152092 -1.09107278], objective = 0.8525854018966292\n",
      "Iteration 4: x = [1.0926595  1.05426158], constraints = [ 0.30537225 -0.03839792 -1.05426158], objective = 0.9030069411562056\n",
      "Iteration 5: x = [1.11789764 1.11545657], constraints = [ 0.49393851 -0.00244107 -1.11545657], objective = 0.7963169316754178\n",
      "Iteration 6: x = [1.11493987 1.10194745], constraints = [ 0.4573791  -0.01299241 -1.10194745], objective = 0.8197095511510271\n",
      "Iteration 7: x = [1.08650317 1.08209988], constraints = [ 0.35142929 -0.00440329 -1.08209988], objective = 0.8500234260915948\n",
      "Iteration 8: x = [1.09183038 1.0713802 ], constraints = [ 0.33994911 -0.02045018 -1.0713802 ], objective = 0.8707675507612924\n",
      "Iteration 9: x = [1.07615722 1.01870163], constraints = [ 0.19586738 -0.0574556  -1.01870163], objective = 0.96874641497964\n",
      "Iteration 10: x = [1.07496046 1.01333792], constraints = [ 0.18239372 -0.06162254 -1.01333792], objective = 0.9791211381380828\n",
      "Iteration 11: x = [1.07672358 0.99786795], constraints = [ 0.15507412 -0.07885563 -0.99786795], objective = 1.0101551510586455\n",
      "Iteration 12: x = [1.07183285 0.98785285], constraints = [ 0.12467891 -0.08397999 -0.98785285], objective = 1.0296018016821453\n",
      "Iteration 13: x = [1.06441302 0.96825993], constraints = [ 0.07050238 -0.09615309 -0.96825993], objective = 1.068636609624374\n",
      "Iteration 14: x = [1.06398618 0.95949488], constraints = [ 0.05269702 -0.10449131 -0.95949488], objective = 1.086745140792548\n",
      "Iteration 15: x = [1.07058307 0.95596433], constraints = [ 0.0600159  -0.11461874 -0.95596433], objective = 1.0949924563964613\n",
      "Iteration 16: x = [0.87199533 0.88183239], constraints = [-0.46199578  0.00983707 -0.88183239], objective = 1.266683995791282\n",
      "Iteration 17: x = [0.88055146 0.87359453], constraints = [-0.46146172 -0.00695693 -0.87359453], objective = 1.2830572300138625\n",
      "Iteration 18: x = [0.89570563 0.87611269], constraints = [-0.43013798 -0.01959294 -0.87611269], objective = 1.2740000009832935\n",
      "Iteration 19: x = [0.90299612 0.87235497], constraints = [-0.42359481 -0.03064116 -0.87235497], objective = 1.2809930734279202\n",
      "Iteration 20: x = [0.89070544 0.8662104 ], constraints = [-0.45632336 -0.02449503 -0.8662104 ], objective = 1.2974241515713734\n"
     ]
    }
   ],
   "source": [
    "# Newton method\n",
    "x = jnp.zeros(2)\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "newton_iter = 100\n",
    "alpha = 0.01\n",
    "\n",
    "# set initial value\n",
    "for i in range(epochs):\n",
    "\n",
    "    x_old = x.copy()\n",
    "    x = x - lr * jax.grad(objective)(x)\n",
    "\n",
    "    if jnp.linalg.norm(x - x_old) < 1e-6:\n",
    "        print(f\"Converged after {i} iterations.: {x}\\n\")\n",
    "        break\n",
    "\n",
    "x_lambda = jnp.concatenate((x, jnp.ones(3)))\n",
    "J = jax.jacobian(lagrange, argnums=0)\n",
    "H = jax.hessian(lagrange, argnums=0)\n",
    "\n",
    "for i in range(newton_iter):\n",
    "\n",
    "    dx = jnp.linalg.inv(H(x_lambda)) @ J(x_lambda)\n",
    "\n",
    "    if any(jnp.isnan(dx)):\n",
    "        break\n",
    "    \n",
    "    x_lambda = x_lambda - alpha * dx\n",
    "\n",
    "    print(f\"Iteration {i}: x = {x_lambda[:-3]}, constraints = {constraints(x_lambda[:-3])}, objective = {objective(x_lambda[:-3])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0153256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = [0.13578914 0.33314748], constraints = [-1.87057407  0.19735834 -0.33314748], objective = 3.525257736488507\n",
      "Iteration 1: x = [0.20054193 0.35722084], constraints = [-1.8321762   0.15667891 -0.35722084], objective = 3.337856561799195\n",
      "Iteration 2: x = [0.22946829 0.38104721], constraints = [-1.80214733  0.15157892 -0.38104721], objective = 3.214727247791555\n",
      "Iteration 3: x = [0.25349235 0.36680616], constraints = [-1.80119487  0.11331381 -0.36680616], objective = 3.2245957772115545\n",
      "Iteration 4: x = [0.20771193 0.36061666], constraints = [-1.82681138  0.15290473 -0.36061666], objective = 3.3152981193831836\n",
      "Iteration 5: x = [0.34795582 0.42051656], constraints = [-1.70209257  0.07256075 -0.42051656], objective = 2.9199295518365926\n",
      "Iteration 6: x = [0.43221553 0.43458191], constraints = [-1.6243283   0.00236637 -0.43458191], objective = 2.7729130037782754\n",
      "Iteration 7: x = [0.66269617 0.23051618], constraints = [-1.50769607 -0.43217999 -0.23051618], objective = 3.244846854004007\n",
      "Iteration 8: x = [0.68195829 0.21828655], constraints = [-1.48728387 -0.46367174 -0.21828655], objective = 3.275653331727643\n",
      "Iteration 9: x = [0.75317145 0.20232805], constraints = [-1.39179613 -0.5508434  -0.20232805], objective = 3.2925487730972445\n",
      "Iteration 10: x = [0.76679186 0.20969505], constraints = [-1.36805822 -0.55709681 -0.20969505], objective = 3.2595778420156685\n",
      "Iteration 11: x = [0.76709646 0.25338255], constraints = [-1.3473603  -0.51371391 -0.25338255], objective = 3.10491656833641\n",
      "Iteration 12: x = [0.77216679 0.25280867], constraints = [-1.33984622 -0.51935813 -0.25280867], objective = 3.1045855244655307\n",
      "Iteration 13: x = [0.78124265 0.26901296], constraints = [-1.31729195 -0.51222969 -0.26901296], objective = 3.044170902354507\n",
      "Iteration 14: x = [0.77697266 0.27207821], constraints = [-1.32228693 -0.50489445 -0.27207821], objective = 3.0354549057016107\n",
      "Iteration 15: x = [0.78876143 0.27560485], constraints = [-1.30189737 -0.51315659 -0.27560485], objective = 3.018160381107286\n",
      "Iteration 16: x = [0.80179086 0.30161935], constraints = [-1.26615719 -0.50017151 -0.30161935], objective = 2.923783690336859\n",
      "Iteration 17: x = [0.81033986 0.30474058], constraints = [-1.2504825  -0.50559927 -0.30474058], objective = 2.9098754606464645\n",
      "Iteration 18: x = [0.80068942 0.30910584], constraints = [-1.26335003 -0.49158358 -0.30910584], objective = 2.8988477632026615\n",
      "Iteration 19: x = [0.81709919 0.31209802], constraints = [-1.23494373 -0.50500117 -0.31209802], objective = 2.8824657868514456\n",
      "NaN\n"
     ]
    }
   ],
   "source": [
    "newton_iter = 100\n",
    "alpha = 0.01\n",
    "\n",
    "x_lambda = jax.random.uniform(jax.random.PRNGKey(11668), (5,))\n",
    "J = jax.jacobian(lagrange, argnums=0)\n",
    "H = jax.hessian(lagrange, argnums=0)\n",
    "\n",
    "for i in range(newton_iter):\n",
    "\n",
    "    dx = jnp.linalg.inv(H(x_lambda)) @ J(x_lambda)\n",
    "\n",
    "    if any(jnp.isnan(dx)):\n",
    "        print(\"NaN\")\n",
    "        break\n",
    "    \n",
    "    x_lambda = x_lambda - alpha * dx\n",
    "\n",
    "    print(f\"Iteration {i}: x = {x_lambda[:-3]}, constraints = {constraints(x_lambda[:-3])}, objective = {objective(x_lambda[:-3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 59 iterations.: [0.99999847 1.99999694]\n",
      "\n",
      "Iteration 10: x = [0.80278405 0.88497999], constraints = [-0.57234819  0.08219594 -0.88497999], objective = 1.2821637519840117\n",
      "Iteration 20: x = [0.84416158 0.81399162], constraints = [-0.62480887 -0.03016996 -0.81399162], objective = 1.4309014875230317\n",
      "Iteration 30: x = [0.89007062 0.76998225], constraints = [-0.61490163 -0.12008836 -0.76998225], objective = 1.5250281251127131\n",
      "Iteration 40: x = [0.9263145  0.73423043], constraints = [-0.60284712 -0.19208408 -0.73423043], objective = 1.607602159601633\n",
      "Iteration 50: x = [0.95272283 0.70564037], constraints = [-0.59439088 -0.24708246 -0.70564037], objective = 1.6776019795417627\n",
      "Iteration 60: x = [0.97153163 0.68380781], constraints = [-0.58853318 -0.28772382 -0.68380781], objective = 1.733172337896585\n",
      "Iteration 70: x = [0.98491092 0.66771927], constraints = [-0.58410146 -0.31719165 -0.66771927], objective = 1.7751996262817813\n",
      "Iteration 80: x = [0.99450768 0.65616436], constraints = [-0.58040281 -0.33834331 -0.65616436], objective = 1.8059243815408033\n",
      "Iteration 90: x = [1.00148626 0.648036  ], constraints = [-0.57707462 -0.35345026 -0.648036  ], objective = 1.8278088762721123\n",
      "Iteration 100: x = [1.00665167 0.64243167], constraints = [-0.57393395 -0.36422    -0.64243167], objective = 1.8430360028805446\n"
     ]
    }
   ],
   "source": [
    "# Full Gradient Descent\n",
    "x = jnp.zeros(2)\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "newton_iter = 100\n",
    "alpha = 0.1\n",
    "\n",
    "history = []\n",
    "\n",
    "# set initial value\n",
    "for i in range(epochs):\n",
    "\n",
    "    x_old = x.copy()\n",
    "    x = x - lr * jax.grad(objective)(x)\n",
    "\n",
    "    if jnp.linalg.norm(x - x_old) < 1e-6:\n",
    "        print(f\"Converged after {i} iterations.: {x}\\n\")\n",
    "        break\n",
    "\n",
    "@jax.jit\n",
    "def lagrange_loss(x_lambda:jnp.ndarray) -> float:\n",
    "\n",
    "    return jnp.linalg.norm(jax.jacobian(lagrange)(x_lambda))\n",
    "\n",
    "x_lambda = jnp.concatenate((x, jnp.ones(3)))\n",
    "grad_fn = jax.grad(lagrange_loss)\n",
    "\n",
    "for i in range(newton_iter):\n",
    "\n",
    "    x_lambda_old = x_lambda.copy()\n",
    "\n",
    "    x_lambda = x_lambda - alpha * grad_fn(x_lambda)\n",
    "\n",
    "    if any(jnp.isnan(x_lambda)):\n",
    "        print(f\"NaN encountered at iteration {i}: {x_lambda}\")\n",
    "        print(f\"{x_lambda_old}\")\n",
    "        break\n",
    "    \n",
    "    if (i+1) % (newton_iter // 10) == 0:\n",
    "        print(f\"Iteration {i+1}: x = {x_lambda[:-3]}, constraints = {constraints(x_lambda[:-3])}, objective = {objective(x_lambda)}\")\n",
    "\n",
    "    if jnp.linalg.norm(x_lambda - x_lambda_old) < 1e-6:\n",
    "        print(f\"Converged after {i} iterations.: {x_lambda}, objective = {objective(x_lambda)}\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = [-3.34910003 -4.14693838], constraints = [26.41356894 -0.79783834  4.14693838], objective = 56.69952251405348\n",
      "Iteration 1: x = [3465.74939546 3832.03356846], constraints = [ 2.66958981e+07  3.66284173e+02 -3.83203357e+03], objective = 26673645.50884811\n",
      "Iteration 2: x = [-3.00564560e+12 -3.10388655e+12], constraints = [ 1.86680172e+25 -9.82409489e+10  3.10388655e+12], objective = 1.8668017194913408e+25\n",
      "Iteration 3: x = [8.89519322e+31 9.07137000e+31], constraints = [ 1.61414216e+64  1.76176789e+30 -9.07137000e+31], objective = 1.6141421609558777e+64\n",
      "Iteration 4: x = [-1.59604661e+73 -1.61021110e+73], constraints = [ 5.14014459e+146 -1.41644913e+071  1.61021110e+073], objective = 5.1401445944796296e+146\n",
      "Iteration 5: x = [4.91355862e+157 4.93644861e+157], constraints = [             inf  2.28899885e+155 -4.93644861e+157], objective = inf\n",
      "Iteration 6: x = [-inf -inf], constraints = [inf nan inf], objective = inf\n",
      "[1. 1. 1. 1. 1.]\n",
      "[nan nan nan nan nan]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# https://tm23forest.com/contents/bfgs-formula-quasi-newton-method-scipy-motivated\n",
    "\n",
    "newton_iter = 20\n",
    "alpha = 1.0\n",
    "\n",
    "dobjdx = jax.jacobian(objective, argnums=0)\n",
    "dconstdx = jax.jacobian(constraints, argnums=0)\n",
    "\n",
    "def f(arr:jnp.ndarray) -> jnp.array:\n",
    "\n",
    "    x, lambda_ = arr[:2], arr[2:]\n",
    "\n",
    "    return jnp.block([dobjdx(x) + lambda_ @ dconstdx(x),\n",
    "                      constraints(x)\n",
    "                      ])\n",
    "\n",
    "# find x, lambda s.t. f(x, lambda) = 0\n",
    "arr = jax.random.uniform(jax.random.PRNGKey(0), (5,))\n",
    "\n",
    "dfdarr = jax.jacobian(f, argnums=0) # 5x5 matrix\n",
    "B = jnp.eye(len(arr))   # Hessian approximation, 5x5 matrix\n",
    "I = jnp.eye(len(arr))   # Identity matrix, 5x5 matrix\n",
    "\n",
    "c1 = 1e-4\n",
    "alphas = jnp.full((len(arr), ), 1.0)    # 1x5 vector\n",
    "\n",
    "for k in range(newton_iter):\n",
    "\n",
    "    grad = dfdarr(arr)\n",
    "\n",
    "    if jnp.linalg.norm(grad) < 0.001: break\n",
    "\n",
    "    p = -B @ grad   # p: 5x5 matrix\n",
    "\n",
    "    gammas = alphas # gammas: 1x5 vector\n",
    "\n",
    "    while all(f(arr + gammas @ p) > f(arr) + c1 * gammas @ grad @ p): gammas *= 0.9\n",
    "\n",
    "    s = gammas @ p  # s: 1x5 vector\n",
    "\n",
    "    if any(jnp.isnan(s)):\n",
    "        print(gammas)\n",
    "        print(rho)\n",
    "        print(grad)\n",
    "        print(p)\n",
    "        print(y)\n",
    "        print(B)\n",
    "        break\n",
    "\n",
    "    arr = arr + s\n",
    "    y = dfdarr(arr) - grad\n",
    "\n",
    "    rho = 1 / (y.T@s)\n",
    "\n",
    "    B = (I - rho * s@y.T) @ B @ (I - rho * y @ s.T) + rho * s @ s.T\n",
    "\n",
    "    print(f\"Iteration {k}: x = {arr[:2]}, constraints = {constraints(arr[:2])}, objective = {objective(arr[:2])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: x = [1.09110191 0.79647743], constraints = [-0.80949662 -0.92900078 -1.43085374], objective = 1.4567661249364345\n",
      "Iteration 200: x = [1.09589966 0.78889083], constraints = [-0.79900393 -0.92935757 -1.41123957], objective = 1.4759821666745985\n",
      "Iteration 300: x = [1.10032121 0.78183848], constraints = [-0.78929323 -0.92975414 -1.39310988], objective = 1.4939818399995595\n",
      "Iteration 400: x = [1.10438774 0.77528842], constraints = [-0.78032772 -0.93017145 -1.37636054], objective = 1.5108152660910976\n",
      "Iteration 500: x = [1.10812456 0.76921354], constraints = [-0.77205996 -0.93060049 -1.36090301], objective = 1.5265262349623514\n",
      "Iteration 600: x = [1.11155532 0.76358755], constraints = [-0.76444478 -0.93103371 -1.3466535 ], objective = 1.5411603336816802\n",
      "Iteration 700: x = [1.11470207 0.75838499], constraints = [-0.7574393  -0.93146487 -1.33353279], objective = 1.5547643900354056\n",
      "Iteration 800: x = [1.11758541 0.75358127], constraints = [-0.75100285 -0.93188887 -1.321466  ], objective = 1.56738597923488\n",
      "Iteration 900: x = [1.12022456 0.74915267], constraints = [-0.74509694 -0.93230161 -1.31038238], objective = 1.5790729953796967\n",
      "Iteration 1000: x = [1.12263745 0.74507636], constraints = [-0.73968516 -0.93269987 -1.30021515], objective = 1.5898732811386265\n"
     ]
    }
   ],
   "source": [
    "# Differential Multiplier\n",
    "# https://github.com/crowsonkb/mdmm-jax/tree/master\n",
    "@jax.jit\n",
    "def eq_constraints(x:jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.asarray([\n",
    "        (x[0]**2 + x[1]**2 - 2 - x[2]**2),\n",
    "        (-x[0] + x[1] - x[3]**2),\n",
    "        (-x[1] - x[4]**2)\n",
    "    ])\n",
    "\n",
    "x = jax.random.uniform(jax.random.PRNGKey(42), (5,))\n",
    "lambda_ = jax.random.uniform(jax.random.PRNGKey(42), (3,))\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.01\n",
    "c = 10\n",
    "\n",
    "dfdx = jax.jacobian(objective, argnums=0)\n",
    "dgdx = jax.jacobian(eq_constraints, argnums=0)\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    dLdx = -dfdx(x) - lambda_ @ dgdx(x) - c * eq_constraints(x) @ dgdx(x)\n",
    "    dLdlambda = eq_constraints(x)\n",
    "\n",
    "    if any(jnp.isnan(dLdx)) or any(jnp.isnan(dLdlambda)):\n",
    "        print(f\"Iteration {i+1}: x = {x[:-3]}, constraints = {eq_constraints(x[:-3])}, objective = {objective(x[:-3])}\")\n",
    "        break\n",
    "\n",
    "    x = x + lr * dLdx\n",
    "    lambda_ = lambda_ + lr * dLdlambda\n",
    "\n",
    "    if (i+1) % (epochs // 10) == 0:\n",
    "        print(f\"Iteration {i+1}: x = {x[:-3]}, constraints = {eq_constraints(x[:-3])}, objective = {objective(x[:-3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70d04de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適解 x* = [1. 1.]\n",
      "乗数 λ*  = [0.50000004 1.00000008 0.        ]\n",
      "最終 μ    = 10000000000.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxopt import LBFGS\n",
    "from functools import partial\n",
    "\n",
    "@jax.jit\n",
    "def objective(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"目的関数 f(x).\"\"\"\n",
    "    return (x[0] - 1.0)**2 + (x[1] - 2.0)**2\n",
    "\n",
    "@jax.jit\n",
    "def constraints(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"不等式制約 g(x) ≤ 0 をベクトルで返す.\"\"\"\n",
    "    return jnp.array([\n",
    "        x[0]**2 + x[1]**2 - 2.0,  # g1(x) ≤ 0\n",
    "        -x[0] + x[1],             # g2(x) ≤ 0\n",
    "        -x[1]                     # g3(x) ≤ 0\n",
    "    ])\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3, 4, 5))\n",
    "def solve_augmented_lagrangian(x0, λ0, μ0, num_outer, num_inner, μ_factor):\n",
    "    x = x0\n",
    "    λ = λ0\n",
    "    μ = μ0\n",
    "\n",
    "    for _ in range(num_outer):\n",
    "        # Augmented Lagrangian の定義\n",
    "        def aug_obj(x):\n",
    "            g = constraints(x)\n",
    "            # max(g, -λ/μ) で「ペナルティをかける部分」を構築\n",
    "            penalty_term = jnp.sum(jnp.maximum(g, -λ/μ)**2)\n",
    "            return objective(x) + λ @ g + (μ * 0.5) * penalty_term\n",
    "\n",
    "        # L-BFGS を用いて内部最適化\n",
    "        solver = LBFGS(fun=aug_obj, maxiter=num_inner)\n",
    "        sol = solver.run(x)\n",
    "        x = sol.params\n",
    "\n",
    "        # multipliers 更新\n",
    "        g = constraints(x)\n",
    "        λ = jnp.maximum(λ + μ * g, 0.0)\n",
    "\n",
    "        # penalty weight を増加\n",
    "        μ = μ * μ_factor\n",
    "\n",
    "    return x, λ, μ\n",
    "\n",
    "# 初期値\n",
    "x0       = jnp.array([0.0, 0.0])\n",
    "λ0       = jnp.zeros(3)\n",
    "μ0       = 1.0\n",
    "num_outer = 10    # 外側ループ回数\n",
    "num_inner = 100   # 内部 L-BFGS の反復数\n",
    "μ_factor  = 10.0  # μ の増大係数\n",
    "\n",
    "x_opt, λ_opt, μ_opt = solve_augmented_lagrangian(\n",
    "    x0, λ0, μ0, num_outer, num_inner, μ_factor\n",
    ")\n",
    "\n",
    "print(\"最適解 x* =\", x_opt)\n",
    "print(\"乗数 λ*  =\", λ_opt)\n",
    "print(\"最終 μ    =\", μ_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
